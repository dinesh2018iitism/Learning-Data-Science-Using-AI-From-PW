{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ0OcG2H/YIYvCnCn6Vtzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinesh2018iitism/Learning-Data-Science-Using-AI-From-PW/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a parameter?"
      ],
      "metadata": {
        "id": "bC-z2BL1o9XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A parameter in machine learning is a value that the model learns during training.\n",
        "#These values help the model make predictions. For example, in a line equation\n",
        "# ùë¶ =ùëöùë• + ùëè\n",
        "# m (slope) and\n",
        "# b (intercept) are parameters.\n",
        "\n",
        "# The training process adjusts parameters to fit the data better."
      ],
      "metadata": {
        "id": "-RHsdLPlo-69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?"
      ],
      "metadata": {
        "id": "zhTnbtDSpEZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation is a measure of how two variables are related.\n",
        "# It shows whether changes in one variable are linked to changes in another.\n",
        "\n",
        "# Positive correlation: Both variables increase or decrease together\n",
        "# e.g. more study time, higher scores\n",
        "\n",
        "# Negative correlation: One variable increases while the other decreases\n",
        "# e.g., more exercise, lower weight\n",
        "\n",
        "# Negative correlation means as one variable goes up, the other goes down.\n",
        "# Example: As the speed of a car increases, the time to reach a destination decreases."
      ],
      "metadata": {
        "id": "EuGYRXHMpMAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "YM7V6icgpenL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning is a field of computer science where machines learn patterns\n",
        "# from data to make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "# Main Components:\n",
        "# Data: The information used to train the model.\n",
        "# Model: The algorithm or method used to learn patterns from data.\n",
        "# Features: The input variables or attributes used to make predictions.\n",
        "# Training: The process of teaching the model using data.\n",
        "# Evaluation: Testing how well the model works on unseen data.\n",
        "# Prediction: The output or result from the trained model."
      ],
      "metadata": {
        "id": "JrfUZZf5piQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "VsD7VYmGpkxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss value measures how far the model's predictions are from the actual results.\n",
        "\n",
        "# Low loss: The model predicts well.\n",
        "# High loss: The model predicts poorly.\n",
        "# During training, the model adjusts to minimize the loss.\n",
        "# A consistently low loss on test data means the model performs well."
      ],
      "metadata": {
        "id": "PJJf2Z8OppS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "rDDNy0gvpqPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuous variables: Variables with values that can take any number within a range\n",
        "# e.g., height, weight, temperature\n",
        "# Example: 5.1, 20.4, 100.7.\n",
        "\n",
        "# Categorical variables: Variables with distinct groups or categories\n",
        "# e.g., gender, color, type\n",
        "# Example: Male/Female, Red/Blue/Green."
      ],
      "metadata": {
        "id": "oPCkYm2fptzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "hfeIVrQbpwq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We handle categorical variables by converting them into numbers\n",
        "#since models work with numeric data.\n",
        "\n",
        "# Common Techniques:\n",
        "# Label Encoding: Assign a unique number to each category.\n",
        "# Example: Red ‚Üí 0, Green ‚Üí 1, Blue ‚Üí 2.\n",
        "\n",
        "# One-Hot Encoding: Create separate columns for each category with 0 or 1 values.\n",
        "# Example:\n",
        "\n",
        "# Red\t    Green\t     Blue\n",
        "# 1\t       0\t        0\n",
        "# 0\t       1\t        0\n",
        "\n",
        "# Ordinal Encoding: Assign numbers based on order if categories have a ranking.\n",
        "# Example: Low ‚Üí 1, Medium ‚Üí 2, High ‚Üí 3.\n",
        "\n",
        "# These techniques depend on the problem and the model used."
      ],
      "metadata": {
        "id": "L2BPLvnmp0IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "arH7GWVWp1C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset: The data used to teach the model.\n",
        "# The model learns patterns and adjusts its parameters using this data.\n",
        "\n",
        "# Testing dataset: Separate data used to check how well the model works on unseen data.\n",
        "# It evaluates the model's performance."
      ],
      "metadata": {
        "id": "0bz8hCwAp6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "k53POtGBp6yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn.preprocessing is a module in Scikit-learn used to prepare data for machine learning.\n",
        "# It helps transform raw data into a format suitable for models.\n",
        "\n",
        "# Common Tools in sklearn.preprocessing:\n",
        "\n",
        "# StandardScaler: Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "# MinMaxScaler: Scales data to a specific range\n",
        "# LabelEncoder: Converts categorical labels into numbers.\n",
        "# OneHotEncoder: Converts categories into binary columns\n",
        "# Binarizer: Converts values into 0 or 1 based on a threshold."
      ],
      "metadata": {
        "id": "HJDGpalap-bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "gh99B68zp_t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A test set is a portion of the data that is used to evaluate the performance of a\n",
        "#  machine learning model after it has been trained.\n",
        "\n",
        "# It is kept separate from the training data to check how well the model\n",
        "# generalizes to new, unseen data. The test set helps in measuring\n",
        "# accuracy and other metrics to see if the model is effective or overfitting."
      ],
      "metadata": {
        "id": "vi_kTz-_qCY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem"
      ],
      "metadata": {
        "id": "Gk5M6TX0qDJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can split data using train_test_split() from Scikit-learn.\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X: features, y: target\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# test_size=0.2: 20% of the data will be used for testing, and the rest (80%) for training.\n",
        "# random_state=42: Ensures the split is reproducible.\n",
        "\n",
        "# Approach to a Machine Learning Problem:\n",
        "\n",
        "# Define the Problem: Understand the task.\n",
        "# Collect and Prepare Data: Gather relevant data and clean it.\n",
        "# Split the Data: Divide the data into training and testing sets.\n",
        "# Choose a Model: Select an appropriate machine learning algorithm.\n",
        "# Train the Model: Use the training data to fit the model.\n",
        "# Evaluate the Model: Test the model using the test set and check metrics.\n",
        "# Tune Hyperparameters: Adjust model settings for better performance.\n",
        "# Deploy the Model: Use the model to make predictions on new data."
      ],
      "metadata": {
        "id": "0UyDptypqGhs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "rswhf_E7qHSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) is crucial before fitting a model\n",
        "#because it helps us understand the data and make better decisions.\n",
        "\n",
        "# Understand Data Structure: EDA helps us to know the types of data, distributions, and relationships between features.\n",
        "# Check for Missing Values: You can identify and handle missing data, which can affect model performance.\n",
        "# Detect Outliers: Outliers can skew results, so detecting them early helps in deciding how to handle them.\n",
        "# Feature Relationships: It helps you explore correlations or dependencies between features, which can guide feature selection or engineering.\n",
        "# Identify Data Imbalances: EDA can highlight imbalances in the target variable, helping us to choose the right model or apply techniques like resampling.\n",
        "# Assumptions Check: Many models have assumptions about the data. EDA helps verify if these assumptions hold, improving model accuracy."
      ],
      "metadata": {
        "id": "E4GcGCvcqQlj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. REPEATED"
      ],
      "metadata": {
        "id": "uiXcCokDqmpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.REPEATED"
      ],
      "metadata": {
        "id": "ba3hknBNqsV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "GyNVNZWzqZPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To find the correlation between variables in Python, we can use the corr() method in Pandas.\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('DATA.csv')\n",
        "# correlation_matrix = df.corr()\n",
        "\n",
        "# print(correlation_matrix)\n",
        "\n",
        "# Explanation:\n",
        "# df.corr(): Computes the Pearson correlation coefficient, which ranges from -1 (perfect negative correlation)\n",
        "# to 1 (perfect positive correlation). 0 means no correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "9abewBubqax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "fC5YZdNuqbqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Causation means that one event directly causes another to happen.\n",
        "# In other words, a change in one variable leads to a change in another variable.\n",
        "\n",
        "# Difference between Correlation and Causation:\n",
        "\n",
        "# Correlation: Two variables are related, but one does not necessarily cause the other.\n",
        "# Causation: One variable directly causes a change in the other.\n",
        "\n",
        "# Example:\n",
        "# Correlation: There is a positive correlation between ice cream sales and the number of people who swim.\n",
        "# As ice cream sales increase, so do swimming activities.\n",
        "# But: This doesn't mean ice cream causes people to swim. Both are related to summer.\n",
        "\n",
        "# Causation: Smoking causes lung cancer.\n",
        "# There is clear evidence that smoking directly leads to a higher risk of developing lung cancer."
      ],
      "metadata": {
        "id": "m-c6TgZMqiEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "ZosxTr3bqwl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An optimizer is an algorithm that helps improve a model by adjusting its parameters to reduce errors.\n",
        "\n",
        "# Types of Optimizers:\n",
        "\n",
        "# Gradient Descent: Updates parameters based on the direction of the loss gradient. It helps minimize errors.\n",
        "\n",
        "# Example: Used in linear regression to adjust weights.\n",
        "\n",
        "# Stochastic Gradient Descent (SGD): Updates parameters after each training example, making it faster but noisier.\n",
        "\n",
        "# Example: Commonly used in training neural networks.\n",
        "\n",
        "# Mini-batch Gradient Descent: Uses a small batch of data to update parameters, balancing speed and accuracy.\n",
        "\n",
        "# Example: Used in deep learning for efficient training.\n",
        "\n",
        "# Momentum: Adds previous gradients to speed up training and avoid bouncing.\n",
        "\n",
        "# Example: Helps neural networks converge faster.\n",
        "\n",
        "# Adam: Combines Momentum and RMSprop, adapting the learning rate for each parameter.\n",
        "\n",
        "# Example: Popular in deep learning.\n",
        "\n",
        "# RMSprop: Divides the learning rate by an average of past squared gradients, working well with noisy data.\n",
        "\n",
        "# Example: Used in training RNNs or time-series models."
      ],
      "metadata": {
        "id": "aoyDGWDLq0aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "pZBLXBxCq1Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn.linear_model is a module in Scikit-learn that provides linear models for machine\n",
        "# learning tasks, such as regression and classification. These models assume a linear relationship\n",
        "#  between the input features and the target variable.\n",
        "\n",
        "# Common Models in sklearn.linear_model:\n",
        "\n",
        "# LinearRegression: Used for predicting continuous values.\n",
        "# Example: Predicting house prices based on features like size, location, etc.\n",
        "\n",
        "\n",
        "# LogisticRegression: Used for binary classification tasks.\n",
        "# Example: Predicting if an email is spam or not.\n",
        "\n",
        "# Ridge: A variation of linear regression that adds regularization (penalty) to reduce overfitting.\n",
        "# Example: Used when features are highly correlated.\n",
        "\n",
        "# Lasso: Similar to Ridge, but uses L1 regularization (penalizes the absolute value of coefficients).\n",
        "# Example: Useful when you want to shrink some coefficients to zero and select important features.\n",
        "\n",
        "# ElasticNet: Combines both Lasso and Ridge regularization.\n",
        "# Example: Used when you need both L1 and L2 regularization."
      ],
      "metadata": {
        "id": "PVg9G-Jrq5FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "ZIjZtqRrq51-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit() is a method in machine learning used to train the model on the data.\n",
        "# It adjusts the model‚Äôs parameters to learn patterns from the input data.\n",
        "\n",
        "# Arguments for model.fit():\n",
        "\n",
        "# X: The input features (independent variables). It should be a 2D array or DataFrame.\n",
        "# y: The target variable (dependent variable). It should be a 1D array or Series.\n",
        "\n",
        "\n",
        "# Example:\n",
        "\n",
        "# model.fit(X_train, y_train)\n",
        "# X_train: Features used to train the model.\n",
        "# y_train: The correct labels or target values.\n",
        "\n",
        "# After calling fit(), the model is trained and ready to make predictions."
      ],
      "metadata": {
        "id": "_D5QZoUmq965"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "XN56Ew1Qq_EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict() is a method used to make predictions on new, unseen\n",
        "# data after the model has been trained with model.fit().\n",
        "\n",
        "# Arguments for model.predict():\n",
        "\n",
        "# X: The input features (independent variables) of the data we want to predict.\n",
        "# It should be the same format (shape) as the training data.\n",
        "\n",
        "# Example:\n",
        "# predictions = model.predict(X_test)\n",
        "# X_test: The features of the test data or new data you want the model to make predictions on.\n",
        "\n",
        "# The method returns predicted values for each sample in X_test based on the trained model."
      ],
      "metadata": {
        "id": "LWEb3pMwrD4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "HY8Z-ROorEg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##REPEATED"
      ],
      "metadata": {
        "id": "OoVriuR8rLzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "uHBVBghcrMnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling is the process of standardizing or normalizing the range of independent variables (features) in your dataset.\n",
        "# This ensures that no feature dominates due to its larger scale, making the model's training more effective.\n",
        "\n",
        "# Common Types of Feature Scaling:\n",
        "# Standardization (Z-score normalization): Centers the data around 0 with a standard deviation of 1.\n",
        "\n",
        "# Formula:\n",
        "# X_scaled= X‚àíŒº / ùúé\n",
        "# where\n",
        "# Œº is the mean and œÉ is the standard deviation.\n",
        "\n",
        "# Normalization (Min-Max scaling): Scales the data to a fixed range, usually 0 to 1.\n",
        "\n",
        "# Formula:\n",
        "# ùëã-scaled= X‚àíX_min / X_max‚àíX_min\n",
        "\n",
        "# where\n",
        "# X-min and ùëã_max are the minimum and maximum values of the feature.\n",
        "\n",
        "# How It Helps in Machine Learning:\n",
        "\n",
        "# Faster convergence: Helps optimization algorithms like gradient descent converge faster.\n",
        "# Improves model performance: Some models, like K-Nearest Neighbors (KNN) or\n",
        "# Support Vector Machines (SVM), are sensitive to the scale of features.\n",
        "# Avoids bias: Features with larger ranges don‚Äôt dominate the learning process."
      ],
      "metadata": {
        "id": "jvVgp8ghrPlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "ZiqdetZXrQTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# In Python, we can perform feature scaling using Scikit-learn's preprocessing module\n",
        "\n",
        "# 1. Standardization (Z-score normalization) with StandardScaler:\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.datasets import load_iris\n",
        "# data = load_iris()\n",
        "# X = data.data\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# print(X_scaled)\n",
        "\n",
        "# fit_transform(): Fits the scaler on the data and then transforms it.\n",
        "\n",
        "# 2. Normalization (Min-Max scaling) with MinMaxScaler:\n",
        "\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# print(X_scaled)\n",
        "\n",
        "# This scales each feature to a range between 0 and 1.\n",
        "\n",
        "# When to Use:\n",
        "# StandardScaler is best when the data follows a Gaussian distribution.\n",
        "# MinMaxScaler is useful when you need a fixed range eg 0 to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "v4QK2-SurTp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "qhmucUkXrUPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REPEATED"
      ],
      "metadata": {
        "id": "y1nwvZsVrZCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "V8DE60lWrZ51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To split data into training and testing sets in Python,\n",
        "# We can use the train_test_split() function from Scikit-learn.\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv('Mydata.csv')\n",
        "\n",
        "# X = df.drop('target_column', axis=1)\n",
        "# y = df['target_column']\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# print(X_train.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "N8NooJv3rdqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?"
      ],
      "metadata": {
        "id": "Kv4zRZyCreWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data encoding is the process of converting categorical data (non-numerical values)\n",
        "# into numerical values so that machine learning models can understand and process them.\n",
        "\n",
        "# Common Types of Data Encoding:\n",
        "\n",
        "# Label Encoding:\n",
        "# Converts each category into a unique integer.\n",
        "# Example:\n",
        "# ['red', 'green', 'blue'] ‚Üí [0, 1, 2]\n",
        "# Used for ordinal data where the categories have a meaningful order.\n",
        "\n",
        "\n",
        "# One-Hot Encoding:\n",
        "# Converts each category into a binary column (1 or 0), where each category gets its own column.\n",
        "# Example:\n",
        "# ['red', 'green', 'blue'] ‚Üí\n",
        "# red  green  blue\n",
        "# 1    0     0\n",
        "# 0    1     0\n",
        "# 0    0     1\n",
        "# Used for nominal data where there is no inherent order\n",
        "\n",
        "# Binary Encoding:\n",
        "# Converts categories into binary digits (for larger categorical variables).\n",
        "# Example: If there are 4 categories, they can be encoded into 2 binary digits:\n",
        "# 0 ‚Üí 00\n",
        "# 1 ‚Üí 01\n",
        "# 2 ‚Üí 10\n",
        "# 3 ‚Üí 11\n",
        "\n",
        "# Target Encoding:\n",
        "# Maps categories to the mean of the target variable for each category.\n",
        "# Example: If the target variable is sales, target encoding will replace each category with the average sales for that category.\n",
        "\n",
        "\n",
        "# How Data Encoding Helps:\n",
        "\n",
        "# Machine Learning Models: Many algorithms (like regression, decision trees, neural networks) require numerical inputs, so encoding categorical data is necessary.\n",
        "# Performance: Encoding helps algorithms learn patterns in categorical data more effectively."
      ],
      "metadata": {
        "id": "QtrCFWYFrjxf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
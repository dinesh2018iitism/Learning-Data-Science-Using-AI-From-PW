{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuxX8buYS46CnwkYofphMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinesh2018iitism/Learning-Data-Science-Using-AI-From-PW/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkvuwHbsS0Z6"
      },
      "outputs": [],
      "source": [
        "# 1. What is Simple Linear Regression?\n",
        "\n",
        "# Simple Linear Regression finds the relationship between two variables:\n",
        "# one dependent (Y) and one independent (X).\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "# A linear relationship exists between X and Y.\n",
        "# Residuals (errors) are normally distributed.\n",
        "# Homoscedasticity: Errors have constant variance.\n",
        "# Residuals are independent of each other.\n",
        "\n",
        "\n",
        "# 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "# The coefficient m is the slope, showing the change in Y for a 1-unit increase in X.\n",
        "\n",
        "# 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "# The intercept c is the value of Y when X=0.\n",
        "\n",
        "# 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "# The slope m is calculated as:\n",
        "# m= Cov(X,Y) / Var(X)\n",
        "\n",
        "# Where Cov(X, Y) is the covariance and Var(X) is the variance of X.\n",
        "\n",
        "# 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "# The least squares method minimizes the sum of squared differences between the actual and predicted\n",
        "# Y, ensuring the best fit line.\n",
        "\n",
        "\n",
        "# 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "# R² explains how much of the variation in Y is explained by X.\n",
        "# It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "\n",
        "\n",
        "# 8. What is Multiple Linear Regression?\n",
        "# Multiple Linear Regression is a method to model the relationship between one dependent variable (\n",
        "# Y) and two or more independent variables (X1, X2,…).\n",
        "\n",
        "# 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "# Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses multiple independent variables.\n",
        "\n",
        "# 10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "# Linearity: The relationship between X and  Y is linear.\n",
        "# Independence of errors.\n",
        "# Homoscedasticity: Constant variance of residuals.\n",
        "# No multicollinearity: Independent variables are not highly correlated.\n",
        "# Residuals are normally distributed.\n",
        "\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "# Heteroscedasticity occurs when the variance of residuals is not constant.\n",
        "# It can lead to unreliable estimates and invalid hypothesis tests.\n",
        "\n",
        "\n",
        "# 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "# Remove highly correlated variables.\n",
        "# Use dimensionality reduction techniques like PCA.\n",
        "# Use regularization methods like Ridge or Lasso Regression.\n",
        "\n",
        "\n",
        "# 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "# One-hot encoding.\n",
        "# Label encoding.\n",
        "# Binary encoding.\n",
        "# Ordinal encoding for ordered categories.\n",
        "\n",
        "\n",
        "\n",
        "# 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "# Interaction terms capture the combined effect of two or more variables on the dependent variable.\n",
        "\n",
        "\n",
        "\n",
        "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "# In Simple Linear Regression, the intercept is the value of Y when X=0.\n",
        "# In Multiple Linear Regression, the intercept represents the value of Y when all independent variables are 0.\n",
        "\n",
        "\n",
        "\n",
        "# 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "# The slope shows how much the dependent variable (Y) changes for a 1-unit increase in the independent variable (X).\n",
        "\n",
        "\n",
        "\n",
        "# 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "# The intercept acts as the baseline value of  Y when all X variables are 0, providing context for predictions.\n",
        "\n",
        "\n",
        "# 18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "# R² doesn't measure accuracy, only fit.\n",
        "# High R² doesn't mean causation.\n",
        "# It can increase with irrelevant variables.\n",
        "\n",
        "\n",
        "\n",
        "# 19. How would you interpret a large standard error for a regression coefficient?\n",
        "# A large standard error indicates that the coefficient is uncertain, suggesting a weak relationship between the variable and the target.\n",
        "\n",
        "# 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "# Identified when residuals show a cone-shaped pattern in scatter plots.\n",
        "# It must be addressed to improve accuracy and ensure valid hypothesis tests.\n",
        "\n",
        "\n",
        "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "# It indicates that some variables in the model don't add significant value, causing overfitting.\n",
        "\n",
        "\n",
        "\n",
        "# 22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "# Scaling ensures all variables have equal importance, especially for algorithms sensitive to magnitude differences.\n",
        "\n",
        "\n",
        "\n",
        "# 23. What is polynomial regression?\n",
        "\n",
        "# Polynomial Regression models the relationship between X and Y using higher-degree polynomial terms of\n",
        "# X.\n",
        "\n",
        "\n",
        "# 24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "# Linear Regression models straight-line relationships, while Polynomial Regression fits curves.\n",
        "\n",
        "# 25. When is polynomial regression used?\n",
        "# It is used when the relationship between variables is non-linear.\n",
        "\n",
        "# 26. What is the general equation for polynomial regression?\n",
        "\n",
        "# Y=b0 + b1 X+b2 X2+⋯+bn Xn +ϵ\n",
        "\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "# Yes, by including polynomial terms for each variable and their interactions.\n",
        "\n",
        "# 28. What are the limitations of polynomial regression?\n",
        "\n",
        "# Overfitting with high-degree polynomials.\n",
        "# Sensitive to outliers.\n",
        "# Harder to interpret.\n",
        "\n",
        "\n",
        "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "# Cross-validation.\n",
        "# R² and Adjusted R².\n",
        "# Mean Squared Error (MSE).\n",
        "\n",
        "\n",
        "\n",
        "# 30. Why is visualization important in polynomial regression?\n",
        "\n",
        "\n",
        "# Visualization helps in understanding the curve's fit and spotting underfitting or overfitting.\n",
        "\n",
        "\n",
        "\n",
        "# 31. How is polynomial regression implemented in Python?\n",
        "# Using PolynomialFeatures from sklearn.preprocessing and LinearRegression."
      ]
    }
  ]
}